import datetime
import gc
from glimr.keras import keras_optimizer
import inspect
import os
import psutil
from ray import tune
from ray.air import CheckpointConfig
from ray.air.config import FailureConfig, RunConfig
from ray.tune import CLIReporter, JupyterNotebookReporter, SyncConfig
from ray.tune.integration.keras import TuneReportCheckpointCallback
from ray.tune.schedulers import AsyncHyperBandScheduler
from ray.tune.stopper import TrialPlateauStopper
from ray.tune.tune_config import TuneConfig
from ray.tune.search.basic_variant import BasicVariantGenerator
import tensorflow as tf
import subprocess
import warnings


class Search(object):
    """A general-purpose class for automated hyperparameter tuning.

    This class provides hyperparameter tuning for any model that has a
    search space and model builder function. Initialization sets reasonable
    options for checkpointing, reporting, stopping criteria, and trial
    resources. Each of these is represented by an attribute and can be
    overrided using class methods. These methods provide a minimal interface
    for a subset of ray parameters, but additional parameters can be set
    using kwargs. Class attributes can also be overrided directly using
    ray library objects generated by the user.

    Parameters
    ----------
    space : dict
        A model search space that defines the range of possible model
        characteristics like architecture, activations, dropout, losses,
        and loss weights.
    builder : callable
        A function that returns a tf.keras.model given a configuration
        sampled from the search space.
    loader : callable
        A function that returns batched training and validation datasets
        of type tf.data.Dataset. This function should include a "batch"
        keyword argument and may include other keyword arguments to control
        data loading and preprocessing behavior.
    stopper : ray.tune.Stopper
        Defines the stopping criteria for terminating trials. May be overrided
        by scheduler stopping criteria. Default value of `None` selects ray's
        TrialPlateauStopper.
    metric : str
        The name of the metric to optimize. This is in the form "task_name"
        where "task" is the task name and "name" is the key value of the
        metric to optimize. Default value of `None` selects the first
        metric of the first task. Loss can be used as a tuning metric by
        substituting "loss" for the metric name.
    mode : {"max", "min"}
        Either "max" or "min" indicating whether to maximize or minimize
        the metric. Default value is "max". If using loss for tuning
        "min" should be selected.
    cv_folds : int
        The number of cross validation folds to perform. Each fold will
        run as a separate trial with the same configuration. Requires the
        dataloader function to have `cv_fold_index` and `cv_folds` arguments.
    fit_kwargs : dict
        Keyword arguments for tf.keras.model.fit. Allows customization of
        keras model training options. Default value is None.

    Attributes
    ----------
    metric : str
        The name of the metric or loss to optimize. This is in the form "task_name"
        where "task" is the task name and "name" is the key value of the
        metric to optimize. If tuning against a loss the name is "loss".
    mode : str
        Either "max" or "min" indicating whether to maximize or minimize
        the metric.
    checkpoint_config : ray.air.CheckpointConfig
        Defines checkpointing preferences for saving the best model from each trial.
    failure_config : ray.air.FailureConfig
        Defines behavior for handling failed trials.
    reporter : ray.tune.CLIReporter or ray.tune.JupyterNotebookReporter
        An optional reporter for displaying experiment progress during tuning.
    resources : dict
        Per-worker compute resources.
    stopper : ray.tune.Stopper
        Defines the stopping criteria for terminating trials. May be overrided
        by scheduler stopping criteria.
    sync_config : tune.SyncConfig
        Defines synchronization options for multi-machine experiments.

    Methods
    -------
    set_checkpoints(metric=None, mode="max", num_to_keep=1, **kwargs)
        Set checkpointing behavior by generating a ray.air.CheckpointConfig.
    set_reporter(metrics=None, parameters=None, jupyter=False, sort_by_metric=True,
        max_report_frequency=30, **kwargs)
        Set reporting behavior by generating a reporter object.
    set_resources(resources={"CPU": 1, "GPU": 0})
        Set experiment resources.
    trainable(config)
        Static function for running a trial.
    experiment(local_dir, name=None, num_samples=100, max_concurrent_trials=8,
        scheduler=AsyncHyperBandScheduler, search_alg=None)
        Runs an experiment.
    """

    def __init__(
        self,
        space,
        builder,
        loader,
        stopper=None,
        metric=None,
        mode="max",
        cv_folds=None,
        loader_kwargs=None,
        fit_kwargs=None,
    ):
        # process kwarg parameters
        if fit_kwargs is None:
            fit_kwargs = {}
        if loader_kwargs is None:
            loader_kwargs = {}

        # add builder, loader, kwargs to space and capture as space member
        space["builder"] = builder
        space["fit_kwargs"] = fit_kwargs
        space["loader"] = loader
        self._space = space
        self.cv_folds = cv_folds

        # check if data loader function has `cv_fold_index`, `cv_folds` arguments
        if cv_folds is not None:
            # add data loader arguments to config
            config["cv_fold_index"] = tune.grid_search(list(range(cv_folds)))

            # check that dataloader has required arguments for cross validation
            args = inspect.getfullargspec(loader)[0]
            if "cv_fold_index" in args and "cv_folds" in args:
                pass
            else:
                raise ValueError(
                    (
                        "Cross validation requires data loader `cv_fold_index`, `cv_folds` function "
                        "arguments."
                    )
                )

        # extract default optimization metric - first task & first metric
        if metric is None:
            taskname = list(space["tasks"].keys())[0]
            metricname = list(space["tasks"][taskname].keys())[0]
            self.metric = f"{taskname}_{metricname}"
        else:
            self.metric = metric

        # capture optimization mode - one of {"min", "max"}
        if mode not in ["min", "max"]:
            raise ValueError("mode must be one of 'min', 'max'.")
        self.mode = mode

        # default CheckpointConfig
        self.set_checkpoints(self.metric, self.mode)

        # default FailureConfig
        self.failure_config = FailureConfig(max_failures=5)

        # default resources
        self.set_resources()

        # default trial/experiment stopper
        if stopper is None:
            self.stopper = TrialPlateauStopper(metric=self.metric)
        else:
            if not isinstance(stopper, tune.Stopper):
                raise ValueError(
                    (
                        "stopper must be a ray.tune.Stopper object. For example: "
                        "TrialPlateauStopper, MaximumIterationStopper, ExperimentPlateauStopper, "
                        "TimeOutStopper, etc."
                    )
                )
            self.stopper = stopper

        # default SyncConfig
        self.sync_config = SyncConfig(syncer=None)

    def set_checkpoints(self, metric=None, mode="max", num_to_keep=1, **kwargs):
        """Set checkpointing behavior for saving models.

        This sets the `ray.air.CheckpointConfig` object that is used by
        `ray.air.config.Runconfig`. This controls aspects including the number
        of checkpoints to retain from each trial, and the metric used to select
        the best checkpoint.

        Parameters
        ----------
        metric : str
            The name of the metric to optimize. This is in the form "task_name"
            where "task" is the task name and "name" is the key value of the
            metric to optimize. Default value of `None` selects the first
            metric of the first task. Loss can be used as a tuning metric by
            substituting "loss" for the metric name.
        mode : {"max", "min"}
            Either "max" or "min" indicating whether to maximize or minimize
            the metric. Default value is "max". If using loss for tuning
            "min" should be selected.
        num_to_keep : int
            The number of checkpoints to retain from each trial. Default value
            is 1.

        Notes
        -----
        See https://docs.ray.io/en/latest/ray-air/api/doc/ray.air.CheckpointConfig.html
        for additional details.
        """

        if metric is None:
            metric = self.metric
        self.checkpoint_config = CheckpointConfig(
            checkpoint_score_attribute=metric,
            checkpoint_score_order=mode,
            num_to_keep=num_to_keep,  # saved checkpoints per trial
            **kwargs,
        )

    def set_reporter(
        self,
        metrics=None,
        parameters=None,
        jupyter=True,
        sort_by_metric=True,
        max_report_frequency=30,
        **kwargs,
    ):
        """Creates a reporter to display trial results and progress during tuning.

        Parameters
        ----------
        metrics : list(string)
            A list of metrics or losses to display during tuning. Format as
            `task_metric` where `task` is the task name and `metric` is the metric
            name. If using a loss the format is `task_loss`.
        parameters : dict
            A dictionary of configuration parameters to display during tuning.
            Each key is an index into the configuration dictionary, and each
            value is the name this parameter will be displayed as. Nested
            parameters are indicated using a `/`. Default value of
            `{"optimization/method": "method", "optimization/learning_rate":
            "learning rate"}` will display the
            `config["optimization"]["method"]` as "method" and
            `config["optimization"]["learning_rate"]` as "learning_rate".
        jupyter : bool
            If running trials in Jupyter, selecting `True` will report updates
            in-place. If `False` reports will be periodically appended to
            command line output. Default value is True.
        sort_by_metric : bool
            If `True`, trials will be sorted by the single metric used to rank
            experiments. Default value is True.
        max_report_frequency : int
            The number of seconds between updates. Default value is 30.

        Notes
        -----
        See https://docs.ray.io/en/latest/tune/api/reporters.html for details
        on reporting in ray tune.
        """

        # set metrics using task and metric names
        if metrics is None:
            metrics = []
            for task_name, task in self._space["tasks"].items():
                if isinstance(task["metrics"], dict):
                    metrics.append(f"{task_name}_{task['metrics']['name']}")
                elif isinstance(task["metrics"], list):
                    for metric in task["metrics"]:
                        metrics.append(f"{task_name}_{metric['name']}")
                metrics.append(f"{task_name}_loss")

        # set display parameters
        if parameters is None:
            parameters = {
                "optimization/method": "method",
                "optimization/learning_rate": "learning rate",
            }

        # set reporter kwargs
        reporter_kwargs = {
            "metric_columns": metrics,
            "parameter_columns": parameters,
            "sort_by_metric": sort_by_metric,
            "max_report_frequency": max_report_frequency,
            **kwargs,
        }

        # select juptyer or cli output
        if jupyter:
            self.reporter = JupyterNotebookReporter(**reporter_kwargs)
        else:
            self.reporter = CLIReporter(**reporter_kwargs)

    def set_resources(
        self,
        resources={"CPU": 1, "GPU": 0},
    ):
        """Set available per-worker cpu and gpu resources.

        This sets cpu/gpu resources available to each worker/trial that are passed to `ray.tune.with_resources()`.
        Fractional resources can be used but may cause memory problems and result in failed trials. The number
        of concurrent trials is limited by both available system resources and the `max_concurrent_trials` argument
        (whichever is smaller). If using fractional resources, setting `max_concurrent_trials` to something less
        than what system resources would permit can help limit out-of-memory errors.

        Parameters
        ----------
        resources : dict
            A dictionary with "CPU" and "GPU" fields defining the resources available to each worker. Fractional
            values result in sharing of resources by workers. Default value is `{"CPU": 1, "GPU": 0}`.

        Notes
        -----
        As per the information available at https://docs.ray.io/en/latest/tune/api/doc/ray.tune.with_resources.html,
        while `ray.tune.with_resources` is designed to accept a `ScalingConfig` object, it has been observed that
        passing a `ScalingConfig` object to `ray.tune.with_resources` doe not result in GPU utilization, regardless
        of the configuration values.

        See https://docs.ray.io/en/latest/tune/tutorials/tune-resources.html for details on parallelism in ray.tune
        and how resources are assigned to trials in tune experiments.
        """

        self.resources = resources

    @staticmethod
    def trainable(config):
        """Trains a model from a hyperparameter configuration.

        This function compiles a model from the config and trains using general
        parameters found in self.options. Communication of results is performed
        using the TuneReportCheckpointCallback from ray.tune.integration.keras.

        Parameters
        ----------
        config : dict
            A configuration describing optimization and model hyperparameters
            as well as functions for data loading and model building.
        """

        # avoid entire memory allocation with TensorFlow
        _ = [
            tf.config.experimental.set_memory_growth(gpu, True)
            for gpu in tf.config.list_physical_devices("GPU")
        ]

        # create the model from the config
        model, losses, loss_weights, metrics = config["builder"](config)

        # build optimizer
        optimizer = keras_optimizer(config["optimization"])

        # compile the model with the optimizer, losses, and metrics
        model.compile(
            optimizer=optimizer, loss=losses, loss_weights=loss_weights, metrics=metrics
        )

        # load example data, generate random train/test split
        train_dataset, validation_dataset = config["loader"](**config["data"])

        # epoch reporting of performance metrics - link keras metric names to names displayed in ray report
        def kerasify(task, metric, multitask=False):
            if multitask:
                return f"{task}_{metric}"
            else:
                return f"{metric}"

        report = {}
        for task_name, task in config["tasks"].items():
            if isinstance(task["metrics"], dict):
                metric_names = [task["metrics"]["name"]]
            elif isinstance(task["metrics"], list):
                metric_names = [metric["name"] for metric in task["metrics"]]
            for metric_name in metric_names:
                report[f"{task_name}_{metric_name}"] = "val_" + kerasify(
                    task_name, metric_name, len(model.outputs) > 1
                )
            report[f"{task_name}_loss"] = "val_" + kerasify(
                task_name, "loss", len(model.outputs) > 1
            )
        callback = TuneReportCheckpointCallback(report)

        # train the model for the desired epochs using the call back
        model.fit(
            train_dataset,
            epochs=config["optimization"]["epochs"],
            validation_data=validation_dataset,
            callbacks=[callback],
            verbose=0,
            **config["fit_kwargs"],
        )

        # free up memory (to improve memory consumption with fractional GPUs/CPUs)
        del model, train_dataset, validation_dataset, losses, loss_weights, metrics

        # garbage collection (to improve memory consumption with fractional GPUs/CPUs)
        _ = gc.collect()

        # empty SWAP memory (to improve memory consumption with fractional GPUs/CPUs)
        subprocess.run(["swapoff", "-a"])
        subprocess.run(["swapon", "-a"])

    def experiment(
        self,
        local_dir,
        name=None,
        num_samples=100,
        max_concurrent_trials=8,
        scheduler=None,
        search_alg=None,
    ):
        """Run hyperparameter tuning experiment trials.

        It is recommended to run one experiment per Search instance. A
        single instance can be used for multiple experiments but Ray Tune
        may cause exceptions depending on the conditions.

        Parameters
        ----------
        local_dir : str
            The path to store experiment results including logs and model
            checkpoints.
        name : str
            The name for the experiment. Used by developers to organize
            experimental results. Default value of None uses datetime
            year_month_day_hour_minute_second.
        num_samples : int
            The number of trials to run when tuning a model. Default value 100.
        max_concurrent_trials : int
            The maximum number of trials to run concurrently. Default value 8.
        scheduler : object
            A scheduling algorithm from ray.tune.schedulers that can be used
            to terminate poorly performing trials, to pause trials, to clone
            trials, and to alter hyperparameters of a running trial. Some
            search algorithms do not require a scheduler. Default value is the
            AsyncHyperBandScheduler.
        search_alg : object
            A search algorithm from ray.tune.search for adaptive hyperparameter
            selection. Default value of None results in a random search with
            the AsyncHyperBandScheduler.

        Returns
        -------
        analysis : ray.tune.ResultGrid
            A dictionary describing the tuning experiment outcomes. See the
            documentation of ray.tune.Tuner.fit() and ray.tune.ResultGrid for
            more details.
        """

        # set config name
        if name is None:
            name = f"{datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}"

        # set scheduler if default
        if scheduler is None:
            scheduler = AsyncHyperBandScheduler(
                time_attr="training_iteration",
                max_t=100,
                grace_period=10,
                stop_last_trials=False,
            )

        # create tune config
        tune_kwargs = {}
        tune_kwargs["max_concurrent_trials"] = max_concurrent_trials
        tune_kwargs["metric"] = self.metric
        tune_kwargs["mode"] = "max"
        tune_kwargs["num_samples"] = num_samples
        tune_kwargs["scheduler"] = scheduler

        # Initiate a new actor for each trial to avoid leaks and memory growth
        tune_kwargs["reuse_actors"] = False

        if search_alg is not None:
            if self.cv_folds is not None:
                # `BasicVariantGenerator` seach algorithm required for cross validation
                tune_kwargs["search_alg"] = BasicVariantGenerator(
                    constant_grid_search=True
                )  # `constant_grid_search` must be True for CV.
                warnings.warn(
                    (
                        "Replacing search algorithm with `BasicVariantGenerator` required for "
                        "cross validation."
                    )
                )
            else:
                tune_kwargs["search_alg"] = search_alg
        tune_config = TuneConfig(**tune_kwargs)

        # create run config
        run_kwargs = {}
        run_kwargs["checkpoint_config"] = self.checkpoint_config
        run_kwargs["failure_config"] = self.failure_config
        run_kwargs["local_dir"] = local_dir
        run_kwargs["log_to_file"] = True
        run_kwargs["name"] = name
        run_kwargs["sync_config"] = self.sync_config
        if hasattr(self, "reporter"):
            run_kwargs["progress_reporter"] = self.reporter
        if hasattr(self, "stopper"):
            run_kwargs["stop"] = self.stopper
        run_config = RunConfig(**run_kwargs)

        # add resource config to search space
        if hasattr(self, "resources"):
            trainable = tune.with_resources(self.trainable, self.resources)
        else:
            trainable = self.trainable

        # set flag indicating first experiment was run
        self._first = True

        # run experiment
        tuner = tune.Tuner(
            trainable,
            param_space=self._space,
            tune_config=tune_config,
            run_config=run_config,
        )
        analysis = tuner.fit()

        return analysis

    @staticmethod
    def restore(local_dir, name=None):
        """Restart an interrupted experiment.

        This function can restore an experiment that was halted to complete
        unfinished trials.

        Parameters
        ----------
        local_dir : str
            The path where experiment results are stored. If `name` is None,
            `local_dir` should include the experiment name. Otherwise it is
            the same `local_dir` argument provided when starting the experiment.
        name : str
            The name for the experiment. When an experiment is run, results are
            stored in local_dir/name. If `name` is None, `local_dir` is assumed
            to contain the full local_dir/name path.
        """

        # form path
        if name is None:
            path = local_dir
        else:
            path = os.join(local_dir, name)

        # run restore
        tuner = tune.Tuner.restore(path, Search.trainable)
        analysis = tuner.fit()

        return analysis
