{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b9676b",
   "metadata": {},
   "source": [
    "# Cross validation example\n",
    "\n",
    "Cross validation can provide a better estimate of performance than a single split of your dataset. We have often observed that running Glimr with a single split produces a configuration that is highly overfit to this validation dataset, and that generalizes poorly to independent testing data. Glimr provides tools to perform cross validation to address this.\n",
    "\n",
    "When performing a cross validation, each model configuration is run in multiple trials with different cross-validation folds. Post experiment analysis can be used to identify the model configuration with the best average performance, or to build ensembles of models trained on different portions of the data.\n",
    "\n",
    "Revisiting the MNIST example, we demonstrate the formulation of cross validation dataloaders and the experiment analysis tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f8176",
   "metadata": {},
   "source": [
    "## Create a cross validation data loader\n",
    "\n",
    "Cross validation requires a dataloader that accepts `cv_index` and `cv_folds` arguments that represent the fold index and number of folds. The `Search` class will populate your data search space with these arguments automatically.\n",
    "\n",
    "This data loader below uses stratified k-fold cross validation to build class-balanced folds. Since each trial will run a separate fold, random arguments like the split seed must be fixed across trials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4fb6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def cv_dataloader(batch_size, random_brightness, max_delta, cv_index, cv_folds):\n",
    "    \"\"\"Cross-validation MNIST data loader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        The number of samples to batch.\n",
    "    random_brightness : bool\n",
    "        Whether to apply random brightness augmentation.\n",
    "    max_delta : float\n",
    "        The random brightness augmentation parameter.\n",
    "    cv_index : int\n",
    "        The index of the requested fold.\n",
    "    cv_folds : int\n",
    "        The number of folds in the cross validation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_ds : tf.data.Dataset\n",
    "        A batched training set for fold `cv_index` used to build models.\n",
    "    validation_ds : tf.data.Dataset.\n",
    "        A batched validation set for fold `cv_index` used to evaluate models.\n",
    "    \"\"\"\n",
    "\n",
    "    # load mnist data\n",
    "    train, validation = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "    # combine training, validation sets\n",
    "    merged = (\n",
    "        np.concatenate((train[0], validation[0]), axis=0),\n",
    "        np.concatenate((train[1], validation[1]), axis=0),\n",
    "    )\n",
    "\n",
    "    # flattening function\n",
    "    def mnist_flat(features):\n",
    "        return features.reshape(\n",
    "            features.shape[0], features.shape[1] * features.shape[2]\n",
    "        )\n",
    "\n",
    "    # stratified k-fold cross validation\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=0)\n",
    "    train_index, validation_index = [\n",
    "        (i, o) for (i, o) in skf.split(merged[0], merged[1])\n",
    "    ][cv_index]\n",
    "\n",
    "    # extract features, labels\n",
    "    train_features = tf.cast(mnist_flat(merged[0][train_index]), tf.float32) / 255.0\n",
    "    train_labels = merged[1][train_index]\n",
    "    validation_features = (\n",
    "        tf.cast(mnist_flat(merged[0][validation_index]), tf.float32) / 255.0\n",
    "    )\n",
    "    validation_labels = merged[1][validation_index]\n",
    "\n",
    "    # build datasets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (train_features, {\"mnist\": tf.one_hot(train_labels, 10)})\n",
    "    )\n",
    "    validation_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        (validation_features, {\"mnist\": tf.one_hot(validation_labels, 10)})\n",
    "    )\n",
    "\n",
    "    # batch\n",
    "    train_ds = train_ds.shuffle(len(train_labels), reshuffle_each_iteration=True)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    validation_ds = validation_ds.batch(batch_size)\n",
    "\n",
    "    # apply augmentation\n",
    "    if random_brightness:\n",
    "        train_ds = train_ds.map(\n",
    "            lambda x, y: (tf.image.random_brightness(x, max_delta), y)\n",
    "        )\n",
    "\n",
    "    return train_ds, validation_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2a3d6",
   "metadata": {},
   "source": [
    "# Setting up the search space and model building funciton\n",
    "\n",
    "The search space and model building function are not impacted by the choice to use cross validation. Reuse everything from the starter example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c338990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glimr.keras import keras_losses, keras_metrics\n",
    "from glimr.optimization import optimization_space\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "# layer 1 hyperparameters\n",
    "activations = tune.choice(\n",
    "    [\"elu\", \"gelu\", \"linear\", \"relu\", \"selu\", \"sigmoid\", \"softplus\"]\n",
    ")\n",
    "layer1 = {\n",
    "    \"activation\": activations,\n",
    "    \"dropout\": tune.quniform(0.0, 0.2, 0.05),\n",
    "    \"units\": tune.choice([64, 48, 32, 16]),\n",
    "}\n",
    "\n",
    "# loss hyperparameters\n",
    "loss = tune.choice(\n",
    "    [\n",
    "        {\"name\": \"categorical_hinge\", \"loss\": tf.keras.losses.CategoricalHinge},\n",
    "        {\n",
    "            \"name\": \"categorical_crossentropy\",\n",
    "            \"loss\": tf.keras.losses.CategoricalCrossentropy,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "# metrics for reporting - fixed non-hyperparameters\n",
    "metrics = {\n",
    "    \"name\": \"auc\",\n",
    "    \"metric\": tf.keras.metrics.AUC,\n",
    "    \"kwargs\": {\"from_logits\": True},\n",
    "}\n",
    "\n",
    "# task layer hyperparameters\n",
    "task = {\n",
    "    \"activation\": activations,\n",
    "    \"dropout\": tune.quniform(0.0, 0.2, 0.05),\n",
    "    \"units\": 10,\n",
    "    \"loss\": loss,\n",
    "    \"loss_weight\": (1.0,),\n",
    "    \"metrics\": metrics,\n",
    "}\n",
    "\n",
    "# optimizer search space\n",
    "optimization = optimization_space()\n",
    "\n",
    "# data loader keyword arguments to control loading, augmentation, and batching\n",
    "data = {\n",
    "    \"batch_size\": tune.choice([32, 64, 128]),\n",
    "    \"random_brightness\": tune.choice(\n",
    "        [True, False]\n",
    "    ),  # whether to perform random brightness transformation\n",
    "    \"max_delta\": tune.quniform(0.01, 0.15, 0.01),\n",
    "}\n",
    "\n",
    "\n",
    "# model builder function\n",
    "def builder(config):\n",
    "    def _build_layer(x, units, activation, dropout, name):\n",
    "        x = tf.keras.layers.Dense(units, activation=activation, name=name)(x)\n",
    "        if dropout > 0.0:\n",
    "            x = tf.keras.layers.Dropout(dropout)(x)\n",
    "        return x\n",
    "    input_layer = tf.keras.Input([784], name=\"input\")\n",
    "    x = _build_layer(\n",
    "        input_layer,\n",
    "        config[\"layer1\"][\"units\"],\n",
    "        config[\"layer1\"][\"activation\"],\n",
    "        config[\"layer1\"][\"dropout\"],\n",
    "        \"layer1\",\n",
    "    )\n",
    "    task_name = list(config[\"tasks\"].keys())[0]\n",
    "    output = _build_layer(\n",
    "        input_layer,\n",
    "        config[\"tasks\"][task_name][\"units\"],\n",
    "        config[\"tasks\"][task_name][\"activation\"],\n",
    "        config[\"tasks\"][task_name][\"dropout\"],\n",
    "        task_name,\n",
    "    )\n",
    "    named = {f\"{task_name}\": output}\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=named)\n",
    "    losses, loss_weights = keras_losses(config)\n",
    "    metrics = keras_metrics(config)\n",
    "\n",
    "    return model, losses, loss_weights, metrics\n",
    "\n",
    "\n",
    "# assemble the search space\n",
    "space = {\n",
    "    \"layer1\": layer1,\n",
    "    \"optimization\": optimization_space(),\n",
    "    \"tasks\": {\"mnist\": task},\n",
    "    \"data\": data,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d7afc",
   "metadata": {},
   "source": [
    "# Using Search with `cv_folds`\n",
    "\n",
    "Creating a `Search` instance with the `cv_folds` argument is all that is needed to instruct `ray.tune` to perform a cross validation.\n",
    "\n",
    "Since `cv_folds` trials will be run for each configuration, the total number of trials will be `cv_folds` * `num_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264a521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from glimr.search import Search\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "\n",
    "# pass `cv_folds` parameter to Search for cross validation\n",
    "tuner = Search(space, builder, cv_dataloader, \"mnist_auc\", cv_folds=5)\n",
    "\n",
    "# make a temporary directory to store outputs - cleanup at end\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "# run trials using default settings\n",
    "with contextlib.redirect_stderr(open(os.devnull, \"w\")):\n",
    "    results = tuner.experiment(local_dir=temp_dir.name, name=\"cross_validation\", num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc79b9-01d7-44a9-bf0b-a57187e2a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glimr.analysis import _parse_experiment, _checkpoints, _filter_checkpoints\n",
    "\n",
    "exp_dir = temp_dir.name + \"/cross_validation\"\n",
    "metric = \"mnist_auc\"\n",
    "df = _parse_experiment(exp_dir)\n",
    "rates = [c[\"optimization\"][\"learning_rate\"] for c in list(df[\"config\"])]\n",
    "\n",
    "# add column where configurations are enumerated\n",
    "from copy import deepcopy\n",
    "import json\n",
    "\n",
    "\n",
    "def _enumerate_configs(df):\n",
    "    cleaned = [deepcopy(c) for c in list(df[\"config\"])]\n",
    "    for clean in cleaned:\n",
    "        del clean[\"data\"][\"cv_index\"]\n",
    "    mapping = {}\n",
    "    for clean in cleaned:\n",
    "        if json.dumps(clean) not in mapping.keys():\n",
    "            mapping[json.dumps(clean)] = len(mapping) + 1\n",
    "    print(len(mapping))\n",
    "    df[\"config_enum\"] = [mapping[json.dumps(clean)] for clean in cleaned]\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
